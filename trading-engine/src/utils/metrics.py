"""
Prometheus metrics for observability.

Provides metrics for:
- Agent execution latency
- LLM call duration and token usage
- Market data fallback usage
- Trading cycle outcomes
- Error rates

Usage:
    from src.utils.metrics import (
        agent_latency,
        llm_call_duration,
        record_agent_execution,
    )

    # Record agent execution
    with agent_latency.labels(agent="DataAgent").time():
        result = await agent.run(state)

    # Or use the helper
    async with record_agent_execution("DataAgent"):
        result = await agent.run(state)
"""

import time
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from prometheus_client import Counter, Gauge, Histogram, Info

from src.utils.logging import get_logger

log = get_logger(__name__)


# =============================================================================
# AGENT METRICS
# =============================================================================

agent_latency = Histogram(
    "trading_agent_latency_seconds",
    "Time spent in each agent",
    labelnames=["agent"],
    buckets=(0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0),
)

agent_calls_total = Counter(
    "trading_agent_calls_total",
    "Total number of agent executions",
    labelnames=["agent", "status"],  # status: success, error
)

agent_signals_generated = Counter(
    "trading_agent_signals_total",
    "Total signals generated by DataAgent",
    labelnames=["action"],  # BUY, SELL, HOLD
)


# =============================================================================
# LLM METRICS
# =============================================================================

llm_call_duration = Histogram(
    "trading_llm_call_duration_seconds",
    "Time spent in LLM API calls",
    labelnames=["agent", "model", "provider"],
    buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0, 120.0),
)

llm_calls_total = Counter(
    "trading_llm_calls_total",
    "Total number of LLM API calls",
    labelnames=["agent", "model", "provider", "status"],  # status: success, retry, error
)

llm_retries_total = Counter(
    "trading_llm_retries_total",
    "Total number of LLM retry attempts",
    labelnames=["agent", "reason"],  # reason: rate_limit, server_error, timeout
)

# Approximate token tracking (would need actual response data for accuracy)
llm_estimated_cost = Counter(
    "trading_llm_estimated_cost_usd",
    "Estimated LLM cost in USD (approximate)",
    labelnames=["agent", "model"],
)


# =============================================================================
# MARKET DATA METRICS
# =============================================================================

market_data_requests = Counter(
    "trading_market_data_requests_total",
    "Total market data requests",
    labelnames=["provider", "status"],  # provider: alpaca, yfinance; status: success, error
)

market_data_fallbacks = Counter(
    "trading_market_data_fallbacks_total",
    "Number of times fallback provider was used",
    labelnames=["primary_provider", "fallback_provider"],
)

market_data_latency = Histogram(
    "trading_market_data_latency_seconds",
    "Market data request latency",
    labelnames=["provider", "request_type"],  # request_type: quote, indicators, history
    buckets=(0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0),
)


# =============================================================================
# TRADING CYCLE METRICS
# =============================================================================

cycle_duration = Histogram(
    "trading_cycle_duration_seconds",
    "Total trading cycle duration",
    labelnames=["cycle_type"],  # scheduled, event
    buckets=(1.0, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0),
)

cycle_total = Counter(
    "trading_cycles_total",
    "Total number of trading cycles",
    labelnames=["cycle_type", "status"],  # status: completed, error
)

signals_by_outcome = Counter(
    "trading_signals_by_outcome_total",
    "Signals categorized by final outcome",
    labelnames=["outcome"],  # generated, risk_approved, validated, executed, rejected
)

execute_decisions = Counter(
    "trading_execute_decisions_total",
    "Number of EXECUTE decisions made",
    labelnames=["symbol"],
)


# =============================================================================
# RISK METRICS
# =============================================================================

hard_constraint_violations = Counter(
    "trading_hard_constraint_violations_total",
    "Hard constraint violations by type",
    labelnames=["constraint_type"],  # insufficient_cash, max_position, max_sector, max_positions
)

circuit_breaker_triggers = Counter(
    "trading_circuit_breaker_triggers_total",
    "Circuit breaker activation events",
    labelnames=["reason"],  # drawdown, consecutive_losses, negative_sharpe
)

current_drawdown = Gauge(
    "trading_current_drawdown_percent",
    "Current portfolio drawdown from peak",
)

consecutive_losses = Gauge(
    "trading_consecutive_losses",
    "Current consecutive loss streak",
)


# =============================================================================
# SYSTEM INFO
# =============================================================================

system_info = Info(
    "trading_system",
    "Trading system information",
)


def set_system_info(version: str, environment: str) -> None:
    """Set system information labels."""
    system_info.info({
        "version": version,
        "environment": environment,
    })


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================


@asynccontextmanager
async def record_agent_execution(
    agent_name: str,
) -> AsyncGenerator[None, None]:
    """
    Context manager to record agent execution metrics.

    Usage:
        async with record_agent_execution("DataAgent"):
            result = await agent.run(state)
    """
    start_time = time.perf_counter()
    status = "success"

    try:
        yield
    except Exception:
        status = "error"
        raise
    finally:
        duration = time.perf_counter() - start_time
        agent_latency.labels(agent=agent_name).observe(duration)
        agent_calls_total.labels(agent=agent_name, status=status).inc()


@asynccontextmanager
async def record_llm_call(
    agent_name: str,
    model: str,
    provider: str,
) -> AsyncGenerator[None, None]:
    """
    Context manager to record LLM call metrics.

    Usage:
        async with record_llm_call("DataAgent", "claude-3-5-sonnet", "anthropic"):
            response = await llm.ainvoke(messages)
    """
    start_time = time.perf_counter()
    status = "success"

    try:
        yield
    except Exception as e:
        error_str = str(e).lower()
        if "rate limit" in error_str or "429" in error_str:
            status = "rate_limit"
        elif "500" in error_str or "502" in error_str or "503" in error_str:
            status = "server_error"
        elif "timeout" in error_str:
            status = "timeout"
        else:
            status = "error"
        raise
    finally:
        duration = time.perf_counter() - start_time
        llm_call_duration.labels(
            agent=agent_name,
            model=model,
            provider=provider,
        ).observe(duration)
        llm_calls_total.labels(
            agent=agent_name,
            model=model,
            provider=provider,
            status=status,
        ).inc()


def record_market_data_request(
    provider: str,
    request_type: str,
    duration: float,
    success: bool,
) -> None:
    """Record a market data request."""
    status = "success" if success else "error"
    market_data_requests.labels(provider=provider, status=status).inc()
    market_data_latency.labels(
        provider=provider,
        request_type=request_type,
    ).observe(duration)


def record_fallback_used(primary: str, fallback: str) -> None:
    """Record when a fallback provider was used."""
    market_data_fallbacks.labels(
        primary_provider=primary,
        fallback_provider=fallback,
    ).inc()
    log.warning(
        "market_data_fallback",
        primary_provider=primary,
        fallback_provider=fallback,
    )


def record_cycle_complete(
    cycle_type: str,
    duration: float,
    signals_count: int,
    risk_approved: int,
    validated: int,
    executed: int,
) -> None:
    """Record cycle completion metrics."""
    cycle_duration.labels(cycle_type=cycle_type).observe(duration)
    cycle_total.labels(cycle_type=cycle_type, status="completed").inc()

    # Track signal funnel
    signals_by_outcome.labels(outcome="generated").inc(signals_count)
    signals_by_outcome.labels(outcome="risk_approved").inc(risk_approved)
    signals_by_outcome.labels(outcome="validated").inc(validated)
    signals_by_outcome.labels(outcome="executed").inc(executed)
    signals_by_outcome.labels(outcome="rejected").inc(signals_count - executed)


def record_hard_constraint_violation(constraint_type: str) -> None:
    """Record a hard constraint violation."""
    hard_constraint_violations.labels(constraint_type=constraint_type).inc()
